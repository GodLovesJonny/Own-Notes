one-hot encoding result is independent  
  
"You shall know a word by the company it keeps."(J. R. Firth)    
"Distributional Similarity"    

## word2vec
### Distributed representations of words
Define a model that aims to predict between a center wt and context words in terms of word vectors  

	p(context|Wt)  

which has a loss function, e.g.,   
	    
	J = 1-p(W-t|Wt)   

W-t means words in Wt's context    
Keep adjusting the vector representations of words to minimize the loss   

### Main idea of word2vec
Two algorithms:   
    
+ Skip-grams(SG)
+ Continuous Bag of Words(CBOW)
    
Two (moderately efficient) training methods:
   
+ Hierarchical softmax
+ Negative sampling

### Skip-grams(SG)

	p(Wt+j|Wt)   

Objective function: Maximize the probability of any context word given the current center word    
=> turn to minimize task with Negative Log Likelihood(Cross-entropy loss)   
With one-how Wt+j target, the only term left is the negative probability of the true class.    
Dot Product to representative the similarity   
Softmax function: Standard map from Rv to a probability discrimination   
+ Exponentiate to make positive, while normalize to give probability
   
There are **2** vectors for each word(as **center**/**context**).    

We take k negative samples    
P(w) = U(w)^(3/4) / Z    
The power makes less frequent words be sampled more often    

**Word2vec improves objective function by putting similar words nearby in space**    

### Summary of word2vec
+ Go through each word of the whole corpus
+ Predict surrounding words of each word
+ The captures cooccurrence of words one at a time
+ Why not capture cooccurrence counts directly?
#### Yes we can!
With a co-occurrence matrix X(Before word2vec)
+ 2 options: windows vs full document
+ Window: Similar to word2vec, use window around each word -> captures both syntactic(POS) and semantic information
+ Word-document co-occurrence matrix will give general topics(all sports terms will hace similar entries) leading to "Latent Semantic Analysis"
