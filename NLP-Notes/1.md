one-hot encoding result is independent  

  
"You shall know a word by the company it keeps."(J. R. Firth)    
"Distributional Similarity"    

## word2vec
### Distributed representations of words
Define a model that aims to predict between a center wt and context words in terms of word vectors  

	p(context|Wt)  

which has a loss function, e.g.,   
	    
	J = 1-p(W-t|Wt)   

W-t means words in Wt's context    
Keep adjusting the vector representations of words to minimize the loss   

### Main idea of word2vec
Two algorithms:   
+ Skip-grams(SG)
+ Continuous Bag of Words(CBOW)
Two (moderately efficient) training methods:
+ Hierarchical softmax
+ Negative sampling
