## Window Classification
Classifying single words is rarely done.   
Interesting problems like ambiguity arise in context.   
**Idea:** classify a word in its context window of neighboring words.   
+ Train softmax classifier by assigning a label to a center word and concatenating all word vectors surroundings it.   
+ Resulting vector Xwindow = x > R(5d), a colunm vector.
+ Use the same softmax classifier with cross entropy

### Updating concatenated vector
+ Short answer: Just take derivatives as before
+ Long answer: Go over steps together

### Sotfmax(=logistic regression) is not very powerful
+ Softmax only linear decision boundaries  
+ Lame when problem is complex

### Nueral Network = running several logistic regressions at the same time

### The max-margin loss
Idea for training objective: make score of the true window(S) larger and **c**orrupt window's score(Sc) lower(until they're good enough):minize   
	
	J = max(0, 1 - S + Sc)

This is continuous --> we can use SGD    
+ xxx | <-- 1 --> | ooo
