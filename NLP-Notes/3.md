## Two views of linguistic structure:
+ Constituency = Phrase structure grammer = context-free grammer(CFGs)
+ Dependency structure

### Attachment ambiguities

### The rise of annotated data: Universial Dependencies treebanks
Starting off building a treebank seems a lot slower and less useful than building a grammer    
But a treebank gives us many things:    
+ Reusability of the labor
+ Broad coverage, not just a few intuitions
+ Frequencies and distributional information
+ A way to evaluate systems

### Dependency Grammar and Dependency Structure
Syntactic structure consists of relations between lexical items, normally binary asymmetric relations("arrows") called **dependencies**.   
The arrows are commonly **typed** with the name of grammatical relations.    
Some people draw the arrows one way, some the other way.   
Usually add a fake Root so every word is a dependent of precisely 1 other node.    

### What are the sources of information for dependency parsing?
+ Bilexical affinities [discussion -> issues] is plausible
+ Dependency distance	mostly with nearby words
+ Intervening material(dependencies rarely span intervening verbs or punctuation)
+ Valency of heads(How many dependents on which side are usual for a head?)

### Depencenvy Parsing
+ A sentence is parsed by choosing for each word what other word(including ROOT) is it a dependent of.
+ Usually some constraints:
  
	Only one word is a dependent of ROOT   
	Don't want cycles A -> B, B -> A   

+ This makes the depencencies a tree
+ Final issue is whether arrow can cross(non-objective) or not

### Methods of Dependency Parsing
+ Dynamic programming
+ Graph algorithms
+ Constraint Satisfaction
+ “Transition-based parsing” or “deterministic dependency parsing”
