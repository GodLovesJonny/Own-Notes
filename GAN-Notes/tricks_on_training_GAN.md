# Tricks on Train Generative Adversaral Networks

1. 更大的kernel，更多的filter
大的kernel能够覆盖更多的像素，也因此能够获得过多的信息。在CIFAR-10数据上，使用5\*5的kernel能够得到很好的效果，而使用3\*3的kernel会导致判别器的损失函数快速跌落至0。对于生成器，顶层更大的kernel某种程度上来说能够保证平滑，对于底层改变kernel的大小不会有什么影响    
filter数目的增加将极大增加网络的参数量，通常来说，filter的数目越多越好。当使用很少的filter时，尤其生成器包含很少的filter时，生成的图像会特别模糊。所以，更多的filter能够获取更多的信息，并最终保证生成的图像具有足够的清晰度    

2. 标签翻转(Generated=True, Real=False)

3. 更改样本标签
在训练判别器时相当有用。0-1标签，即hard label，可能会使得判别器的损失值迅速跌落至0。所以改用soft label，对于标签为0的样本，其标签设置为(0, 0.1)内的随机数；对于标签为1的样本，其标签设置为(0.9, 1)内的随机数，当然，这个操作在训练生成器时不需要做    
在训练样本的标签上加入随机噪声，也能提升模型效果。如对于进入判别器中5%的图像，随机翻转其标签，也能提升一些效果   

4. Batch Normalization
加入BN能够使得生成的图像更加细腻，但当kernel和filter的数目设置不对，或者判别器的损失函数快速跌落至0时，增加BN没什么作用   

5. 一次训练一种类型的图片
为了能够让训练更加简单，有必要使得输入数据具有相似的特点。比如，使用CIFAR-10中所有10种类型的样本训练，不如挑选其中一类，比如汽车或者青蛙，训练模型专门生成这个类别的图像。当然，也有一些DC-GAN的其他变种，能够同时学习多个类别的图像。比如，Conditional GANs，以标签为输入并生成基于类别标签的图像，但是当使用简单的DC-GAN时，最好构造一类图像数据，让训练变得简单   

6. 注意训练过程中的梯度
训练过程中应关注梯度和损失值，这些信息能够帮助我们了解训练的进展情况，甚至能够帮助我们定位问题代码    
理想情况下，在训练开始时，生成器产生的梯度通常比较大，而判别器的梯度不一定很大，因为开始的时候，判别器能够很容易分清真实和虚假的图片。当生成器训练到一定程度时，能够生成一些“以假乱真”的图片，这会使得判别器很难区分真实和虚假的图片，此时判别器会犯错，同时梯度也会很大    

7. 不要Early Stopping
这是刚刚接触GAN的训练时一个常见的错误：当发现损失值不变时，或者生成的图像一直模糊时，通常会终止训练，调整模型。但这个时候也要注意一下，GAN的训练通常非常耗时，所以有时候多等一等会有意想不到的“收获”    
值得注意的是，当判别器的损失值快速接近0时，通常生成器很难学到任何东西了，就需要及时终止训练，修改网络、重新训练    
