one-hot encoding result is independent  
  
"You shall know a word by the company it keeps."(J. R. Firth)    
"Distributional Similarity"    

## word2vec
### Distributed representations of words
Define a model that aims to predict between a center wt and context words in terms of word vectors  

	p(context|Wt)  

which has a loss function, e.g.,   
	    
	J = 1-p(W-t|Wt)   

W-t means words in Wt's context    
Keep adjusting the vector representations of words to minimize the loss   

### Main idea of word2vec
Two algorithms:   
    
+ Skip-grams(SG)
+ Continuous Bag of Words(CBOW)
    
Two (moderately efficient) training methods:
   
+ Hierarchical softmax
+ Negative sampling

### Skip-grams(SG)

	p(Wt+j|Wt)   

Objective function: Maximize the probability of any context word given the current center word    
=> turn to minimize task with Negative Log Likelihood(Cross-entropy loss)   
With one-how Wt+j target, the only term left is the negative probability of the true class.    
Dot Product to representative the similarity   
Softmax function: Standard map from Rv to a probability discrimination   
+ Exponentiate to make positive, while normalize to give probability
   
There are **2** vectors for each word(as **center**/**context**).    
