one-hot encoding result is independent  
  
"You shall know a word by the company it keeps."(J. R. Firth)    
"Distributional Similarity"    

## word2vec
### Distributed representations of words
Define a model that aims to predict between a center wt and context words in terms of word vectors  

	p(context|Wt)  

which has a loss function, e.g.,   
	    
	J = 1-p(W-t|Wt)   

W-t means words in Wt's context    
Keep adjusting the vector representations of words to minimize the loss   

### Main idea of word2vec
Two algorithms:   
    
+ Skip-grams(SG)
+ Continuous Bag of Words(CBOW)
    
Two (moderately efficient) training methods:
   
+ Hierarchical softmax
+ Negative sampling

### Skip-grams(SG)

	p(Wt+j|Wt)   

Objective function: Maximize the probability of any context word given the current center word    
=> turn to minimize task with Negative Log Likelihood(Cross-entropy loss)   
With one-how Wt+j target, the only term left is the negative probability of the true class.    
Dot Product to representative the similarity   
Softmax function: Standard map from Rv to a probability discrimination   
+ Exponentiate to make positive, while normalize to give probability
   
There are **2** vectors for each word(as **center**/**context**).    

We take k negative samples    
P(w) = U(w)^(3/4) / Z    
The power makes less frequent words be sampled more often    

**Word2vec improves objective function by putting similar words nearby in space**    

### Summary of word2vec
+ Go through each word of the whole corpus
+ Predict surrounding words of each word
+ The captures cooccurrence of words one at a time
+ Why not capture cooccurrence counts directly?
#### Yes we can!
With a co-occurrence matrix X(Before word2vec)
+ 2 options: windows vs full document
+ Window: Similar to word2vec, use window around each word -> captures both syntactic(POS) and semantic information
+ Word-document co-occurrence matrix will give general topics(all sports terms will hace similar entries) leading to "Latent Semantic Analysis"

### Problems with simple co-occurrence vectors
+ Increase in size with vocabulary
+ Very high dimensional: require a lot of storage
+ Subsequent classification models have sparsity issues
+ -> Models are less robust

### Solution: Low dimension	vectors
+ Idea: store "most" of the important information in a fixed, small number of dimensions: a dense vector
+ Usually 25-1000 dimensions, similar to word2vec
+ How to reduce the dimensionality?

### Method1: Dimensionality Reduction on X
Singular Value Deconposition of co-occurrence matrix X.     

### Hacks to X
Problems: function words(the, he, has) are too frequent -> syntax has too much impact.    
Some fixes:    
+ min(X, t), with t~100
+ Ignore them all
Rampled windows that count closer words more    
Use Pearson correlations instead of counts, then set negative values to 0    

### Problems with SVD
+ Computational cost scales quadratically for n x m matrix    
+ Hard to incorporate new words or documents
+ Different learning regime than other DL models

### Combining the best of both worlds: GloVe(Global Vector)
+ Fast training
+ Scalable to huge corpora
+ Good performance even with small sorpus, ane small vectors
+ dot-product-logPij

### What to do with the two sets of vectors?
+ Xfinal = U + V

**Skip-gram model tries to capture co-occurrences one window at a time, while the Glove model tries to capture the counts of the overall statistics of how often these word appear together.**    

### What about polysemy?
Word vectors can capture polysemy.        
Word vectors are linear superposition of each sense vector.    
Sense/context vectors can be recovered by sparse coding.    
The senses recovered are about as good as a non-native English speaker.    
